on.exit({
if(exists("cl") && inherits(cl, "cluster")) {
tryCatch({
parallel::stopCluster(cl)
}, error = function(e) {
# 记录错误但不中断
log_message(sprintf("Error stopping cluster: %s", e$message), tag = "WARNING")
})
}
}, add = TRUE)
# 将必要的包加载到每个工作进程
parallel::clusterEvalQ(cl, {
library(dplyr)
library(httr)
library(jsonlite)
})
# 直接将所有需要的函数和变量导出到集群
# 这避免了依赖于包的加载
function_list <- c(
"search_compounds", "process_results", "retry_GET", "get_kegg_id",
"string_similarity", "log_message", "clean_cache"
)
# 获取当前环境中的函数定义
functions_env <- new.env()
for (fn in function_list) {
if (exists(fn, envir = .GlobalEnv)) {
functions_env[[fn]] <- get(fn, envir = .GlobalEnv)
} else if (exists(fn, envir = environment())) {
functions_env[[fn]] <- get(fn, envir = environment())
}
}
# 导出函数和必要的全局变量
parallel::clusterExport(cl, c(names(functions_env), "cache_dir", "APP_PATHS", "verbose"),
envir = environment())
# 处理化合物
log_message("Processing compounds in parallel...")
# 使用并行lapply处理每个化合物
parallel_results <- parallel::parLapply(cl, names_for_step2, function(compound) {
tryCatch({
# 在每个并行工作进程中创建必要的目录
if (!dir.exists(cache_dir)) {
dir.create(cache_dir, recursive = TRUE, showWarnings = FALSE)
}
# 搜索化合物
result <- search_compounds(compound, verbose = verbose)
if (!is.null(result)) {
# 处理结果
processed <- process_results(result, target = compound, verbose = verbose)
# 保存到缓存 - 直接保存processed而不添加额外嵌套
if (!is.null(processed)) {
result_path <- file.path(cache_dir, paste0(make.names(compound), ".rds"))
saveRDS(processed, result_path)
}
# 直接返回processed，不添加额外嵌套
return(processed)
} else {
return(NULL)
}
}, error = function(e) {
# 记录错误但不中断进程
return(list(error = e$message))
})
})
# 先记录结果处理完成，再关闭集群
log_message("Parallel processing complete")
# 给结果命名
names(parallel_results) <- names_for_step2
# 过滤掉NULL结果和有错误的结果
step2_results <- parallel_results[!sapply(parallel_results, function(x) {
is.null(x) || !is.null(x$error)
})]
}, error = function(e) {
log_message(sprintf("Error in parallel processing: %s", e$message), tag = "ERROR")
# 如果并行处理失败，回退到顺序处理
step2_results <- process_until_complete(names_for_step2, cache_dir = cache_dir, verbose = verbose)
})
} else {
# 如果n_cores <= 1，使用顺序处理
log_message("Using sequential processing (n_cores <= 1)")
step2_results <- process_until_complete(names_for_step2, cache_dir = cache_dir, verbose = verbose)
}
}
step1_results <- lapply(split_by_searchname, function(sub_df) {
# Select only the target columns
# Use drop = FALSE to ensure it remains a data frame even if only one row/col
processed_df <- sub_df[, target_cols, drop = FALSE]
# Convert column types to match the target 'results' structure
# Use suppressWarnings for potential NAs introduced by coercion if CID is not numeric
processed_df$CID <- suppressWarnings(as.integer(processed_df$CID))
processed_df$MolecularWeight <- suppressWarnings(as.numeric(processed_df$MolecularWeight))
# Similarity is already numeric in the example, but good practice to ensure
processed_df$Similarity <- as.numeric(processed_df$Similarity)
# Create the final list structure for this SearchName
list(
all_results = processed_df,
top_results = processed_df # Identical content as requested
)
})
# 变量保存并行处理结果
step2_results <- NULL
# Check and load required parallel processing packages
if (!requireNamespace("parallel", quietly = TRUE)) {
log_message("Package 'parallel' is required for parallel processing. Using sequential processing instead.",
tag = "WARNING")
step2_results <- process_until_complete(names_for_step2, cache_dir = cache_dir, verbose = verbose)
} else {
# Set up parallel processing
if (n_cores > 1) {
log_message(sprintf("Setting up parallel processing with %d cores", n_cores))
# 创建一个新的作用域来处理集群并确保正确关闭
tryCatch({
cl <- parallel::makeCluster(n_cores)
on.exit({
if(exists("cl") && inherits(cl, "cluster")) {
tryCatch({
parallel::stopCluster(cl)
}, error = function(e) {
# 记录错误但不中断
log_message(sprintf("Error stopping cluster: %s", e$message), tag = "WARNING")
})
}
}, add = TRUE)
# 将必要的包加载到每个工作进程
parallel::clusterEvalQ(cl, {
library(dplyr)
library(httr)
library(jsonlite)
})
# 直接将所有需要的函数和变量导出到集群
# 这避免了依赖于包的加载
function_list <- c(
"search_compounds", "process_results", "retry_GET", "get_kegg_id",
"string_similarity", "log_message", "clean_cache"
)
# 获取当前环境中的函数定义
functions_env <- new.env()
for (fn in function_list) {
if (exists(fn, envir = .GlobalEnv)) {
functions_env[[fn]] <- get(fn, envir = .GlobalEnv)
} else if (exists(fn, envir = environment())) {
functions_env[[fn]] <- get(fn, envir = environment())
}
}
# 导出函数和必要的全局变量
parallel::clusterExport(cl, c(names(functions_env), "cache_dir", "APP_PATHS", "verbose"),
envir = environment())
# 处理化合物
log_message("Processing compounds in parallel...")
# 使用并行lapply处理每个化合物
parallel_results <- parallel::parLapply(cl, names_for_step2, function(compound) {
tryCatch({
# 在每个并行工作进程中创建必要的目录
if (!dir.exists(cache_dir)) {
dir.create(cache_dir, recursive = TRUE, showWarnings = FALSE)
}
# 搜索化合物
result <- search_compounds(compound, verbose = verbose)
if (!is.null(result)) {
# 处理结果
processed <- process_results(result, target = compound, verbose = verbose)
# 保存到缓存 - 直接保存processed而不添加额外嵌套
if (!is.null(processed)) {
result_path <- file.path(cache_dir, paste0(make.names(compound), ".rds"))
saveRDS(processed, result_path)
}
# 直接返回processed，不添加额外嵌套
return(processed)
} else {
return(NULL)
}
}, error = function(e) {
# 记录错误但不中断进程
return(list(error = e$message))
})
})
# 先记录结果处理完成，再关闭集群
log_message("Parallel processing complete")
# 给结果命名
names(parallel_results) <- names_for_step2
# 过滤掉NULL结果和有错误的结果
step2_results <- parallel_results[!sapply(parallel_results, function(x) {
is.null(x) || !is.null(x$error)
})]
}, error = function(e) {
log_message(sprintf("Error in parallel processing: %s", e$message), tag = "ERROR")
# 如果并行处理失败，回退到顺序处理
step2_results <- process_until_complete(names_for_step2, cache_dir = cache_dir, verbose = verbose)
})
} else {
# 如果n_cores <= 1，使用顺序处理
log_message("Using sequential processing (n_cores <= 1)")
step2_results <- process_until_complete(names_for_step2, cache_dir = cache_dir, verbose = verbose)
}
}
results <- c(step1_results,step2_results)
View(results)
# 确保结果非空
if (is.null(results)) {
log_message("No results obtained from processing, attempting sequential processing as fallback", tag = "WARNING")
results <- process_until_complete(name_list_unique, cache_dir = cache_dir, verbose = verbose)
}
!is.null(results)
!is.null(results) && length(results) > 0
results_file <- file.path(process_dir, "all_results.rds")
View(results)
valid_results <- Filter(function(x) !is.null(x$all_results), results)
match_table <- create_match_status_table(valid_results)
View(match_table)
load_all()
MetFusionAnnotator::init_app('/Users/zhonghua/Documents/Software_Development/500-test')
run_ui()
load_all()
MetFusionAnnotator::init_app('/Users/zhonghua/Documents/Software_Development/500-test')
run_ui()
run_ui()
load_all()
MetFusionAnnotator::init_app('/Users/zhonghua/Documents/Software_Development/500-test')
run_ui()
load_all()
MetFusionAnnotator::init_app('/Users/zhonghua/Documents/Software_Development/500-test')
run_ui()
system.file("extdata", "cleaned_db.fst", package = "MetFusionAnnotator")
name_db <- tryCatch({
# fst::read_fst('/Users/zhonghua/Documents/Software_Development/cleaned_db.fst')
fst::read_fst(db_path)
log_message('Load backgroud database success...')
}, error = function(e) {
if (verbose) log_message(sprintf("Error loading local database: %s. Proceeding without local DB merge.", e$message), tag = "ERROR")
return(NULL) # Return NULL or an empty df structure expected by merge
})
verbose = F
name_db <- tryCatch({
# fst::read_fst('/Users/zhonghua/Documents/Software_Development/cleaned_db.fst')
fst::read_fst(db_path)
log_message('Load backgroud database success...')
}, error = function(e) {
if (verbose) log_message(sprintf("Error loading local database: %s. Proceeding without local DB merge.", e$message), tag = "ERROR")
return(NULL) # Return NULL or an empty df structure expected by merge
})
db_path
# --- Step 2: Merge with Local Database ---
# Ensure required columns exist in the loaded database for the logic below
# Specifically: 'cleaned_Metabolite_name_lc', 'pubchem_cid', 'inchikey', 'smiles'
db_path <- system.file("extdata", "cleaned_db.fst", package = "MetFusionAnnotator")
name_db <- tryCatch({
# fst::read_fst('/Users/zhonghua/Documents/Software_Development/cleaned_db.fst')
fst::read_fst(db_path)
log_message('Load backgroud database success...')
}, error = function(e) {
if (verbose) log_message(sprintf("Error loading local database: %s. Proceeding without local DB merge.", e$message), tag = "ERROR")
return(NULL) # Return NULL or an empty df structure expected by merge
})
name_db <- tryCatch({
# fst::read_fst('/Users/zhonghua/Documents/Software_Development/cleaned_db.fst')
fst::read_fst(db_path)
}, error = function(e) {
if (verbose) log_message(sprintf("Error loading local database: %s. Proceeding without local DB merge.", e$message), tag = "ERROR")
return(NULL) # Return NULL or an empty df structure expected by merge
})
log_message('Load backgroud database success...')
load_all()
MetFusionAnnotator::init_app('/Users/zhonghua/Documents/Software_Development/500-test')
run_ui()
load_all()
MetFusionAnnotator::init_app('/Users/zhonghua/Documents/Software_Development/500-test')
run_ui()
load_all()
MetFusionAnnotator::init_app('/Users/zhonghua/Documents/Software_Development/500-test')
load_all()
load_all()
MetFusionAnnotator::init_app('/Users/zhonghua/Documents/Software_Development/500-test')
run_ui()
run_ui()
load_all()
a <- readRDS('/Users/zhonghua/Documents/Software_Development/500-test/inst/extdata/KEGG/KEGG_pathways_compound_hsa.rds')
View(a)
load_all()
library(devtools)
load_all()
MetFusionAnnotator::init_app('/Users/zhonghua/Documents/Software_Development/500-test')
run_ui()
a <- readRDS('/Users/zhonghua/Documents/Software_Development/500-test/inst/extdata/KEGG/KEGG_pathways_compound_mmu.rds')
b <- readRDS('/Users/zhonghua/Documents/Software_Development/500-test/inst/extdata/KEGG/KEGG_pathways_compound_hsa.rds')
View(b)
identical(a$compound_KEGG_ID, b$compound_KEGG_ID)
run_ui()
c <- readRDS('/Users/zhonghua/Documents/Software_Development/500-test/inst/extdata/KEGG/KEGG_pathways_compound_pps.rds.rds')
c <- readRDS('/Users/zhonghua/Documents/Software_Development/500-test/inst/extdata/KEGG/KEGG_pathways_compound_pps.rds')
View(c)
identical(a$compound_KEGG_ID, c$compound_KEGG_ID)
run_ui()
d <- readRDS('/Users/zhonghua/Documents/Software_Development/500-test/inst/extdata/KEGG/KEGG_pathways_compound_cata.rds')
identical(a$compound_KEGG_ID, d$compound_KEGG_ID)
??kegg_compound_data()
?kegg_compound_data()
?kegg_compound_data
species_code = 'hsa'
log_message(sprintf("Processing %s ...", species_code))
# Create KEGG directory if it doesn't exist
kegg_dir <- file.path(APP_PATHS$data, "KEGG")
dir.create(kegg_dir, recursive = TRUE, showWarnings = FALSE)
# Record the directory creation status
log_message(sprintf("KEGG directory path: %s", kegg_dir))
log_message(sprintf("KEGG directory exists: %s", dir.exists(kegg_dir)))
if (!requireNamespace("KEGGREST", quietly = TRUE)) {
stop("Package 'KEGGREST' is required but not installed. Please install it manually.")
}
species_path <- retry_kegg_request(keggLink, "pathway", species_code)
species_path[1]
species_path[2]
# Filter meta pathways
meta <- unique(species_path)[grepl(paste0(species_code, '00'), unique(species_path))]
if (length(meta) == 0) {
log_message(sprintf("No pathways found for %s. Skipping...", species_code))
return(NULL)
}
# Get pathway info in batches
batch_size <- 10
n_batches <- ceiling(length(meta) / batch_size)
species_info <- vector("list", length(meta))
# Get pathway info in batches
batch_size <- 10
n_batches <- ceiling(length(meta) / batch_size)
species_info <- vector("list", length(meta))
start_idx <- (i-1) * batch_size + 1
i =1
start_idx <- (i-1) * batch_size + 1
end_idx <- min(i * batch_size, length(meta))
current_batch <- meta[start_idx:end_idx]
current_batch
batch_info <- lapply(current_batch, function(m) {
Sys.sleep(0.1)  # Add small delay to avoid too rapid requests
retry_kegg_request(keggGet, m)
})
species_code = 'mmu'
log_message(sprintf("Processing %s ...", species_code))
# Create KEGG directory if it doesn't exist
kegg_dir <- file.path(APP_PATHS$data, "KEGG")
dir.create(kegg_dir, recursive = TRUE, showWarnings = FALSE)
# Record the directory creation status
log_message(sprintf("KEGG directory path: %s", kegg_dir))
log_message(sprintf("KEGG directory exists: %s", dir.exists(kegg_dir)))
if (!requireNamespace("KEGGREST", quietly = TRUE)) {
stop("Package 'KEGGREST' is required but not installed. Please install it manually.")
}
species_path <- retry_kegg_request(keggLink, "pathway", species_code)
# Filter meta pathways
meta <- unique(species_path)[grepl(paste0(species_code, '00'), unique(species_path))]
if (length(meta) == 0) {
log_message(sprintf("No pathways found for %s. Skipping...", species_code))
return(NULL)
}
# Get pathway info in batches
batch_size <- 10
n_batches <- ceiling(length(meta) / batch_size)
species_info <- vector("list", length(meta))
start_idx <- (i-1) * batch_size + 1
end_idx <- min(i * batch_size, length(meta))
current_batch <- meta[start_idx:end_idx]
log_message(sprintf("Processing batch %d/%d for %s", i, n_batches, species_code))
batch_info <- lapply(current_batch, function(m) {
Sys.sleep(0.1)  # Add small delay to avoid too rapid requests
retry_kegg_request(keggGet, m)
})
run_ui()
e <- readRDS('/Users/zhonghua/Documents/Software_Development/500-test/inst/extdata/KEGG/KEGG_pathways_compound_ath.rds')
species_code = 'ath'
log_message(sprintf("Processing %s ...", species_code))
# Create KEGG directory if it doesn't exist
kegg_dir <- file.path(APP_PATHS$data, "KEGG")
dir.create(kegg_dir, recursive = TRUE, showWarnings = FALSE)
# Record the directory creation status
log_message(sprintf("KEGG directory path: %s", kegg_dir))
log_message(sprintf("KEGG directory exists: %s", dir.exists(kegg_dir)))
if (!requireNamespace("KEGGREST", quietly = TRUE)) {
stop("Package 'KEGGREST' is required but not installed. Please install it manually.")
}
species_path <- retry_kegg_request(keggLink, "pathway", species_code)
# Filter meta pathways
meta <- unique(species_path)[grepl(paste0(species_code, '00'), unique(species_path))]
if (length(meta) == 0) {
log_message(sprintf("No pathways found for %s. Skipping...", species_code))
return(NULL)
}
# Get pathway info in batches
batch_size <- 10
n_batches <- ceiling(length(meta) / batch_size)
species_info <- vector("list", length(meta))
for(i in 1:n_batches) {
start_idx <- (i-1) * batch_size + 1
end_idx <- min(i * batch_size, length(meta))
current_batch <- meta[start_idx:end_idx]
log_message(sprintf("Processing batch %d/%d for %s", i, n_batches, species_code))
batch_info <- lapply(current_batch, function(m) {
Sys.sleep(0.1)  # Add small delay to avoid too rapid requests
retry_kegg_request(keggGet, m)
})
species_info[start_idx:end_idx] <- batch_info
}
# Extract pathway names
nm <- unlist(lapply(species_info, function(x) x[[1]]$NAME))
# Extract compound information
compounds <- unlist(lapply(species_info, function(x) {
g <- x[[1]]$COMPOUND
if (length(g) > 0) {
compound_names <- as.character(g)
compound_ids <- names(g)
combined <- paste(compound_ids, compound_names, sep=": ")
paste(combined, collapse='; ')
} else {
NA
}
}))
# Create compounds dataframe
df_compounds <- data.frame(
species = species_code,
pathway_id = meta,
pathway_name = nm,
compounds = compounds,
stringsAsFactors = FALSE
)
species_code
meta
nm
length(nm)
species_info[[1]]
species_info[[1]]$NAME
species_info[[1]]
View(species_info)
species_info[[1]][[1]][["NAME"]]
# Extract pathway names
# nm <- unlist(lapply(species_info, function(x) x[[1]]$NAME))
nm <- unlist(lapply(species_info, function(x) x[[1]][[1]]$NAME))
# Extract pathway names
# nm <- unlist(lapply(species_info, function(x) x[[1]]$NAME))
nm <- unlist(lapply(species_info, function(x) x[[1]]$NAME))
which(nm == 'NAME')
nm
species_info[[100]][[1]][["NAME"]]
species_info[[100]][[1]][["NAME"]][1]
species_info[[1]][[1]][["NAME"]][1]
# Extract pathway names
# nm <- unlist(lapply(species_info, function(x) x[[1]]$NAME))
nm <- unlist(lapply(species_info, function(x) x[[1]]$NAME[1]))
# Extract compound information
compounds <- unlist(lapply(species_info, function(x) {
g <- x[[1]]$COMPOUND
if (length(g) > 0) {
compound_names <- as.character(g)
compound_ids <- names(g)
combined <- paste(compound_ids, compound_names, sep=": ")
paste(combined, collapse='; ')
} else {
NA
}
}))
# Create compounds dataframe
df_compounds <- data.frame(
species = species_code,
pathway_id = meta,
pathway_name = nm,
compounds = compounds,
stringsAsFactors = FALSE
)
View(df_compounds)
library(devtools)
load_all()
init_app('/Users/zhonghua/Documents/Software_Development/500-test')
run_ui()
load_all()
init_app('/Users/zhonghua/Documents/Software_Development/500-test')
run_ui()
run_ui()
load_all()
init_app('/Users/zhonghua/Documents/Software_Development/500-test')
run_ui()
run_ui()
APP_PATHS$process
list.files(APP_PATHS$process)
load_all()
load_all()
load_all()
load_all()
load_all()
load_all()
load_all()
load_all()
list.files('inst/www/')
load_all()
init_app('/Users/zhonghua/Documents/Software_Development/500-test')
run_ui()
load_all()
init_app('/Users/zhonghua/Documents/Software_Development/500-test')
run_ui()
devtools::load_all()
init_app('/Users/zhonghua/Documents/Software_Development/500-test')
run_ui()
library(devtools)
load_all()
init_app('/Users/zhonghua/Documents/Software_Development/500-test')
run_ui()
load_all()
init_app('/Users/zhonghua/Documents/Software_Development/500-test')
run_ui()
load_all()
init_app('/Users/zhonghua/Documents/Software_Development/500-test')
run_ui()
load_all()
library(devtools)
load_all()
init_app('/Users/zhonghua/Documents/Software_Development/500-test')
run_ui()
